{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;(])\"\n",
    "PAD = '<PAD>'\n",
    "STD = '<SOS>'\n",
    "END = '<END>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "MAX_SEQUENCE=25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    q, a = list(data_df['Q']), list(data_df['A'])\n",
    "    return q, a\n",
    "\n",
    "inputs, outputs = load_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vocabulary 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0,
     12,
     24,
     30
    ]
   },
   "outputs": [],
   "source": [
    "def data_tokenizer(sentences):\n",
    "    \"\"\"\n",
    "    문장 전처리 후 단어 리스트 리턴  (voca 만들때 사용)\n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def prepare_like_morphlized(sentences):\n",
    "    \"\"\"\n",
    "    형태소 형태로 준비\n",
    "    \"\"\"\n",
    "    \n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        morphed_sentence = ' '.join(okt.morphs(sentence.replace(' ', '')))\n",
    "        result.append(morphed_sentence)\n",
    "    return result\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    word2idx = {word:idx for idx, word in enumerate(vocabulary_list)}\n",
    "    idx2word = {idx:word for idx, word in enumerate(vocabulary_list)}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "def load_vocabulary(data_path, vocab_path, tokenize_as_morph=False):\n",
    "    \n",
    "    # 기 생성 voca가 없으면 생성\n",
    "    if not os.path.exists(vocab_path):\n",
    "        if(os.path.exists(data_path)):\n",
    "            q, a = load_data(data_path)\n",
    "            \n",
    "            # 형태소 형태로 처리할 것인가?\n",
    "            if tokenize_as_morph:\n",
    "                q = prepare_like_morphlized(q)\n",
    "                a = prepare_like_morphlized(a)\n",
    "        \n",
    "        data = []\n",
    "        data.extend(q)\n",
    "        data.extend(a)\n",
    "        words = data_tokenizer(data)\n",
    "        words = list(set(words))\n",
    "        words[:0] = MARKER\n",
    "                \n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocab_file:\n",
    "            for word in words:\n",
    "                vocab_file.write(word+'\\n')\n",
    "    \n",
    "    vocab_list = []\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            vocab_list.append(line.strip())\n",
    "    word2idx, idx2word = make_vocabulary(vocab_list)\n",
    "    \n",
    "    return word2idx, idx2word, len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './chatbot.csv'\n",
    "VOCAB_PATH = './vocabulary.txt'\n",
    "    \n",
    "word2idx, idx2word, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 0),\n",
       " ('<SOS>', 1),\n",
       " ('<END>', 2),\n",
       " ('<UNK>', 3),\n",
       " ('참았는데', 4),\n",
       " ('끝내더라', 5),\n",
       " ('솔직히', 6),\n",
       " ('역할을', 7),\n",
       " ('이상하다', 8),\n",
       " ('사귀겠대', 9)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코더 입력 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def enc_processing(sentences, dictionary, tokenize_as_morph=False):\n",
    "    \"\"\"\n",
    "    각 문장에 dictionary를 적용해서 integer sequence로 바꾼다.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences_input_index = []\n",
    "    sentences_length = [] # 실제 문장별 길이\n",
    "    \n",
    "    if tokenize_as_morph:\n",
    "        sentences = prepare_like_morphlized(sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        sentence_index = []\n",
    "        for word in sentence.split():\n",
    "            if dictionary[word] is not None:\n",
    "                sentence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sentence_index.extend([dictionary[UNK]])\n",
    "        \n",
    "        if len(sentence_index) > MAX_SEQUENCE:  # truncate beyond max length\n",
    "            sentence_index = sentence_index[:MAX_SEQUENCE]\n",
    "            \n",
    "        sentences_length.append(len(sentence_index))   # PADDING 전 실제 문장 길이 측정\n",
    "        sentence_index += (MAX_SEQUENCE-len(sentence_index)) * [dictionary[PAD]]   # PAD IF shorter than MAX_LEN\n",
    "        \n",
    "        sentences_input_index.append(sentence_index)\n",
    "        \n",
    "    return np.array(sentences_input_index), sentences_length\n",
    "\n",
    "index_inputs, input_seq_len = enc_processing(\n",
    "    inputs, word2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11828, 16701,   780,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_inputs[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1지망 학교 떨어졌어'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디코더 입력/타겟  만들기\n",
    "\n",
    "<code>\n",
    "  디코더 입력값: SOS    그래        오랜만이야   PAD\n",
    "  디코더 타켓값: 그래   오랜만이야   END         PAD\n",
    "<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dec_output_processing(sentences, dictionary, tokenize_as_morph=False):\n",
    "\n",
    "    sentences_output_index = []\n",
    "    sentences_length = [] # 실제 문장별 길이\n",
    "    \n",
    "    if tokenize_as_morph:\n",
    "        sentences = prepare_like_morphlized(sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        sentence_index = []\n",
    "        \n",
    "        # start with <SOS>\n",
    "        sentence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] \n",
    "                                              for word in sentence.split()]\n",
    "        \n",
    "        if len(sentence_index) > MAX_SEQUENCE:  # truncate beyond max length\n",
    "            sentence_index = sentence_index[:MAX_SEQUENCE]\n",
    "            \n",
    "        sentences_length.append(len(sentence_index))   # PADDING 전 실제 문장 길이 측정\n",
    "        sentence_index += (MAX_SEQUENCE-len(sentence_index)) * [dictionary[PAD]]   # PAD IF shorter than MAX_LEN\n",
    "        \n",
    "        sentences_output_index.append(sentence_index)\n",
    "        \n",
    "    return np.array(sentences_output_index), sentences_length    \n",
    "    \n",
    "def dec_target_processing(sentences, dictionary, tokenize_as_morph=False):\n",
    "\n",
    "    sentences_target_index = []\n",
    "    \n",
    "    if tokenize_as_morph:\n",
    "        sentences = prepare_like_morphlized(sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        sentence_index = []\n",
    "    \n",
    "        sentence_index =[dictionary[word] if word in dictionary else dictionary[UNK] for word in sentence.split()]\n",
    "    \n",
    "        # end with <END>\n",
    "        if len(sentence_index) >= MAX_SEQUENCE:  \n",
    "            sentence_index = sentence_index[:MAX_SEQUENCE-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sentence_index += [dictionary[END]]\n",
    "            \n",
    "        sentence_index += (MAX_SEQUENCE-len(sentence_index)) * [dictionary[PAD]]   # PAD IF shorter than MAX_LEN\n",
    "        sentences_target_index.append(sentence_index)\n",
    "        \n",
    "    return np.array(sentences_target_index)        \n",
    "\n",
    "index_outputs, output_seq_len = dec_output_processing(outputs, word2idx, tokenize_as_morph=False)\n",
    "index_targets = dec_target_processing(outputs, word2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1, 13479, 20262,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_outputs[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'위로해 드립니다.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13479, 20262,     2,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_targets[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['word2idx'] = word2idx\n",
    "data_configs['idx2word'] = idx2word\n",
    "data_configs['vocab_size'] = vocab_size\n",
    "data_configs['pad_symbol'] = PAD\n",
    "data_configs['std_symbol'] = STD\n",
    "data_configs['end_symbol'] = END\n",
    "data_configs['unk_symbol'] = UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "np.save(open(TRAIN_INPUTS, 'wb'), index_inputs)\n",
    "np.save(open(TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
    "np.save(open(TRAIN_TARGETS , 'wb'), index_targets)\n",
    "\n",
    "json.dump(data_configs, open(DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
